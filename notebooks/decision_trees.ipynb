{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual calculation: 0.9544340029249649\n",
      "Scipy calculation: 0.954434002924965\n"
     ]
    }
   ],
   "source": [
    "# Entropy calculation\n",
    "# let's say we have configuration of 5 red balls and 3 blue balls.\n",
    "\n",
    "# The probability for red ball being picked is 5/8\n",
    "# The probability for blue ball being picked is 3/8\n",
    "\n",
    "# formula -> negative of the sum of log with base 2 of probability of picking the balls.\n",
    "# formula -> -(m / m + n * log(m/m+n, 2)) - (n / m + n * log(n / m + n, 2))\n",
    "\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "\n",
    "print(f\"Manual calculation: {- (5 / 8 * math.log(5 / 8, 2)) - (3 / 8 * math.log(3 / 8, 2))}\")\n",
    "\n",
    "print(f\"Scipy calculation: { entropy([5/8, 3/8], base=2) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8631205685666311"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quiz for \"Entropy Formula 3\"\n",
    "# 4 red to 10 blus balls.\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "red = 4\n",
    "blue = 10\n",
    "total = red + blue\n",
    "\n",
    "entropy([red/total, blue/total], base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3346791410515946"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-class Entropy\n",
    "# 4 red to 10 blus balls.\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "red = 8\n",
    "blue = 3\n",
    "yellow = 2\n",
    "\n",
    "total = red + blue + yellow\n",
    "\n",
    "entropy([red/total, blue/total, yellow/total], base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11260735516748954"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# Quiz for Maximizing Information Gain\n",
    "\n",
    "# InformationGain -> Entropy(Parent) âˆ’ (m / m + n * Entropy(Child1) + n / m + n * Entropy(Child2)).\n",
    "\n",
    "import pandas as ps\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "data = ps.read_csv('data/ml-bugs.csv')\n",
    "\n",
    "species = data['Species'].values\n",
    "color = data['Color'].values\n",
    "length = data['Length (mm)'].values\n",
    "\n",
    "lt17_length_split = [species[index] for index, item in enumerate(length) if item < 17]\n",
    "gt17_length_split = [species[index] for index, item in enumerate(length) if item > 17]\n",
    "\n",
    "\n",
    "_, parent_unique_species_count = np.unique(species, return_counts=True)\n",
    "_, first_half_unique_species_count = np.unique(lt17_length_split, return_counts=True)\n",
    "_, second_half_unique_species_count = np.unique(gt17_length_split, return_counts=True)\n",
    "\n",
    "\n",
    "parent_entropy = entropy([ specie / len(species) for specie in parent_unique_species_count], base=2)\n",
    "first_half_entropy = entropy([ specie / len(lt17_length_split) for specie in first_half_unique_species_count], base=2)\n",
    "second_half_entropy = entropy([ specie / len(gt17_length_split) for specie in second_half_unique_species_count], base=2)\n",
    "\n",
    "\n",
    "children_entropy = len(lt17_length_split) / len(species) * first_half_entropy + len(gt17_length_split) / len(species) * second_half_entropy\n",
    "\n",
    "\n",
    "result = parent_entropy - children_entropy\n",
    "\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 1.0)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# Decision Tree Quiz Sklearn\n",
    "\n",
    "# Import statements \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data/decision_tree_sklearn.csv', header=None))\n",
    "\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# TODO: Create the decision tree model and assign it to the variable model.\n",
    "# You won't need to, but if you'd like, play with hyperparameters such\n",
    "# as max_depth and min_samples_leaf and see what they do to the decision\n",
    "# boundary.\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Fit the model.\n",
    "model.fit(X, y)\n",
    "\n",
    "# # TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# # TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "\n",
    "# acc = accuracy_score()\n",
    "\n",
    "acc = accuracy_score(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "The training accuracy is 0.8707865168539326\n",
      "The test accuracy is 0.8547486033519553\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# Lab: Titanic Survival Exploration with Decision Trees\n",
    "\n",
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Set a random seed\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "in_file = 'data/titanic_data.csv'\n",
    "full_data = pd.read_csv(in_file)\n",
    "\n",
    "\n",
    "# Store the 'Survived' feature in a new variable and remove it from the dataset\n",
    "outcomes = full_data['Survived']\n",
    "features_raw = full_data.drop('Survived', axis = 1)\n",
    "\n",
    "# Removing the names\n",
    "features_no_names = features_raw.drop(['Name'], axis = 1)\n",
    "\n",
    "# One-hot encoding\n",
    "features = pd.get_dummies(features_no_names)\n",
    "\n",
    "features = features.fillna(0.0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier(max_depth=6, min_samples_leaf=6)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)\n",
    "\n",
    "# Hyperparameter optimization.\n",
    "\n",
    "# param_dict = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': range(1, 50),\n",
    "#     'min_samples_split': range(1, 10),\n",
    "#     'min_samples_leaf': range(1, 5)\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_dict, cv=10, verbose=1, n_jobs=-1)\n",
    "\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
